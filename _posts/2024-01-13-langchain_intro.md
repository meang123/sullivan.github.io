---
layout: single
title:  "Transformer model ì •ë¦¬ ë° lagnchain ì •ë¦¬"
categories: LLM,langchain,Transformer model,chain,agent,conversational memory,RAG,prompt 
tag : [LLM,langchain,introduction,Transformer,Transformer model,chain,agent,conversational memory,RAG,prompt]
toc: true
author_profile: false
sidebar:
  nav : "docs"
search: true
typora-root-url: ../
---





# ì†Œê°œ 



langchainì„ ê²½í—˜í•´ë³´ë©´ì„œ ì–»ì€ ì •ë³´ë¥¼ ì •ë¦¬ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. langchain ê³µì‹ ë¬¸ì„œì— ì •ë¦¬ê°€ ì˜ ë˜ì–´ ìˆì–´ì„œ ëŒ€ë¶€ë¶„ langchain ê³µì‹ ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ transformer modelì— ëŒ€í•˜ì—¬ ì•Œê³  ìˆìœ¼ë©´ ì¢‹ì„ê²ƒ ê°™ì•„ì„œ ê°™ì´ ì •ë¦¬ë¥¼ í–ˆìŠµë‹ˆë‹¤. 



### ì°¸ê³ ë§í¬ 

ë‹¤ìŒì€ ì£¼ë¡œ ì°¸ê³  í•œ ì˜ìƒ ì£¼ì†Œì™€ ê³µì‹ ë¬¸ì„œ ì£¼ì†Œ ì…ë‹ˆë‹¤ 



[(233) James Briggs - YouTube](https://www.youtube.com/@jamesbriggs)

[(233) Sam Witteveen - YouTube](https://www.youtube.com/@samwitteveenai)



ëª‡ë‹¬ë§Œì— ë˜ ë‹¤ë¥¸ ë‚´ìš©ë“¤ì´ ì¶”ê°€ê°€ ê³„ì† ë˜ê³  ìˆìœ¼ë‹ˆ ìì£¼ ê³µì‹ ì‚¬ì´íŠ¸ ì°¸ê³  í•˜ë©´ ë„ì›€ì´ ë ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë°‘ì—ì„œ ì •ë¦¬í•œ ë‚´ìš© + ë°‘ì— ì‚¬ì´íŠ¸ì—ì„œ ì†Œê°œí•˜ëŠ” use caseì— ëŒ€í•´ì„œ ê°™ì´ ë³´ë©´ ë” í’ë¶€í•œ ë‚´ìš©ì„ ì–»ì„ìˆ˜ìˆì„ê²ƒ ê°™ìŠµë‹ˆë‹¤ 



ê·¸ë¦¬ê³  pdfì— ë‚˜ì™€ìˆëŠ” ì˜ìƒì´ë‚˜ ë¸”ë¡œê·¸ ê¸€ë“¤ì„ ëª¨ë‘ ë³´ê³  ë‚˜ì„œ ì •ë¦¬ í•œê²Œ ì•„ë‹ˆë¼ ê°€ë³ê²Œ ì°¸ê³  í•˜ê±°ë‚˜ ì•ˆë³¸ ì˜ìƒë“¤ë„ ìˆê¸° ë•Œë¬¸ì—(ìë£Œ ì°¾ì•„ë³´ë‹¤ê°€ ì°¸ê³  í•˜ë©´ ì¢‹ì„ê²ƒë“¤ì„ ì •ë¦¬ í–ˆê¸° ë•Œë¬¸) í˜¹ì‹œ ë¬´ì‘ì • ì˜ìƒì— ëŒ€í•œ ìš”ì•½ë„ ê°™ì´ ìˆì„ê²ƒì´ë¼ê³  ìƒê°í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.



[Use cases | ğŸ¦œï¸ğŸ”— Langchain](https://python.langchain.com/docs/use_cases)

[LangChain AI Handbook | Pinecone](https://www.pinecone.io/learn/series/langchain/)





ê·¸ë¦¬ê³  ëª¨ë“  ì •ë¦¬ ë‚´ìš©ì€ pdfë¡œ ì œê³µ ë˜ê¸° ë•Œë¬¸ì— pdfë¡œ ì°¸ê³  í•´ ì£¼ì‹œë©´ ê°ì‚¬ í•˜ê² ìŠµë‹ˆë‹¤ 



------



# Transformer model ì •ë¦¬ 


<a href="{{site.url}}/pdfs/transformers model.pdf">Transformers model PDF</a>



#### Self attention

Query @key transpose **[ë‚´ì ì€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œê°€ ìœ ì‚¬ì„±ì„ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì— ê²°êµ­ Qì™€ Kë“¤ì˜ ìœ ì‚¬ì„±ì„ í™•ì¸í•˜ëŠ” ì‘ì—…ì¸ê²ƒì´ë‹¤] :**    

 ê° chunkì— ëŒ€í•œ score ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©í• ìˆ˜ì—†ìœ¼ë‹ˆ rowë°©í–¥ìœ¼ë¡œ softmaxì ìš©í•œë‹¤ ê·¸ë¦¬ê³  ë‚˜ì„œ Value matrixê³±í•œë‹¤ 

ì˜ˆë¥¼ ë“¤ì–´ I love you allì˜ chunkê°€ ìˆì„ë•Œ Q@key Tì˜ scoreì—ì„œ Ië¼ë¦¬ëŠ” 0.9ì •ë„ì´ê³  I queryì™€ you keyê°’ì˜ softmaxê°’ì´ 0.1ì˜ ìƒê´€ê´€ê³„ê°€ ë‚˜ì™”ë‹¤ê³  í•˜ë©´ softmax(Q@key T) @Vë¥¼ í•´ì¤Œìœ¼ë¡œì¨ 

Iì™€ youì— ëŒ€í•œ ìƒê´€ê´€ê³„ê°€ ì ìš©ëœ ìƒˆë¡œìš´ I ë²¡í„°ë¥¼ í˜•ì„±í•˜ê²Œ ë˜ëŠ” íš¨ê³¼ë¥¼ ê°€ì§„ë‹¤ 



ê° ì¿¼ë¦¬ì— ëŒ€í•œ í‚¤ ì •ë³´ì˜ ê°€ì¤‘ì¹˜ í•©ì´ë‹¤ ì¦‰ ì¿¼ë¦¬ì™€ í‚¤ì™€ì˜ ê´€ë ¨ ì •ë³´ë¥¼ ì–»ê³  í‚¤ì— ëŒ€í•œ valueë¥¼ weithed sumí•œê²ƒì´ attentionì— ëŒ€í•œ ê²°ê³¼ ì´ë‹¤ 

ê°ê°ì˜ ì¿¼ë¦¬ì— ëŒ€í•´ í‚¤ì™€ ë‚´ì  êµ¬í•œë‹¤ : self attention => ë™ìŒì´ì˜ì–´ , ìˆœì„œ ë§¥ë½ì„ êµ¬ë¶„ í• ìˆ˜ìˆë‹¤ 









#### Multi head attention



CNNì—ì„œ ì»¤ë„ ì—¬ëŸ¬ê°œ ë§Œë“¤ì–´ì„œ ì„œë¡œ ë‹¤ë¥¸ í”¼ì³ êµ¬í•˜ëŠ” í•„í„°ë§µ êµ¬í•œê²ƒì²˜ëŸ¼ self attetionì„ ì—¬ëŸ¬ê°œ ë§Œë“¤ì–´ì„œ ê°ê¸° ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ ê°’ì„ ê°€ì§„ attentionì„ concateí•œê²ƒì´ë‹¤ 








# prompt 



<a href="{{site.url}}/pdfs/prompt templates.pdf">prompt PDF</a>


# conversation memory for llm 



<a href="{{site.url}}/pdfs/Conversational memory for LLM.pdf">conversational memory PDF</a>



# retrival augemented generation(RAG)

<a href="{{site.url}}/pdfs/retrival.pdf">retrival PDF</a>



# chain 



<a href="{{site.url}}/pdfs/chain.pdf">chain PDF</a>



# Agent 



<a href="{{site.url}}/pdfs/agent.pdf">Agent PDF</a>
